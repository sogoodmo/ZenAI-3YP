{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import mediapipe "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "    \n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Detections"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "''' Using the media pose model'''\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    '''\n",
    "        Capturing webcam footage \n",
    "    '''\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        suc, frame = cap.read() \n",
    "        if not suc:\n",
    "            print(\"Frame empty..\")\n",
    "            continue \n",
    "        \n",
    "        #Recolor image \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detections \n",
    "        results = pose.process(image)\n",
    "        \n",
    "        #Revert image color \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        \n",
    "        #Render detections \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(50, 145, 168), circle_radius=2, thickness=2),\n",
    "            mp_drawing.DrawingSpec(color=(209, 192, 42), circle_radius=2, thickness=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Displaying the frame \n",
    "        cv2.imshow(\"Video\", cv2.flip(image, 1))\n",
    "        \n",
    "        # Closing the video capture  \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'mp_pose' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' Using the media pose model'''\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmp_pose\u001b[49m\u001b[38;5;241m.\u001b[39mPose(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pose:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Capturing webcam footage \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mp_pose' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Determine Joints \n",
    "- ![x](../assets/33Points.jpg)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "''' Using the media pose model'''\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    '''\n",
    "        Capturing webcam footage \n",
    "    '''\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        suc, frame = cap.read() \n",
    "        if not suc:\n",
    "            print(\"Frame empty..\")\n",
    "            continue \n",
    "        \n",
    "        #Recolor image \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detections \n",
    "        results = pose.process(image)\n",
    "        \n",
    "        #Revert image color \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        #Extract Landmarks\n",
    "        try: \n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "        except:\n",
    "            pass\n",
    "         \n",
    "        #Render detections \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(50, 145, 168), circle_radius=2, thickness=2),\n",
    "            mp_drawing.DrawingSpec(color=(209, 192, 42), circle_radius=2, thickness=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Displaying the frame \n",
    "        cv2.imshow(\"Video\", cv2.flip(image, 1))\n",
    "        \n",
    "        # Closing the video capture  \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'mp_pose' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' Using the media pose model'''\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmp_pose\u001b[49m\u001b[38;5;241m.\u001b[39mPose(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pose:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Capturing webcam footage \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mp_pose' is not defined"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "landmarkStr = {\n",
    "    mp_pose.PoseLandmark.NOSE : \"NOSE\",\n",
    "    mp_pose.PoseLandmark.LEFT_EYE_INNER : \"LEFT_EYE_INNER\",\n",
    "    mp_pose.PoseLandmark.LEFT_EYE : \"LEFT_EYE\",\n",
    "    mp_pose.PoseLandmark.LEFT_EYE_OUTER : \"LEFT_EYE_OUTER\",\n",
    "    mp_pose.PoseLandmark.RIGHT_EYE_INNER : \"RIGHT_EYE_INNER\",\n",
    "    mp_pose.PoseLandmark.RIGHT_EYE : \"RIGHT_EYE\",\n",
    "    mp_pose.PoseLandmark.RIGHT_EYE_OUTER : \"RIGHT_EYE_OUTER\",\n",
    "    mp_pose.PoseLandmark.LEFT_EAR : \"LEFT_EAR\",\n",
    "    mp_pose.PoseLandmark.RIGHT_EAR : \"RIGHT_EAR\",\n",
    "    mp_pose.PoseLandmark.MOUTH_LEFT : \"MOUTH_LEFT\",\n",
    "    mp_pose.PoseLandmark.MOUTH_RIGHT : \"MOUTH_RIGHT\",\n",
    "    mp_pose.PoseLandmark.LEFT_SHOULDER : \"LEFT_SHOULDER\",\n",
    "    mp_pose.PoseLandmark.RIGHT_SHOULDER : \"RIGHT_SHOULDER\",\n",
    "    mp_pose.PoseLandmark.LEFT_ELBOW : \"LEFT_ELBOW\",\n",
    "    mp_pose.PoseLandmark.RIGHT_ELBOW : \"RIGHT_ELBOW\",\n",
    "    mp_pose.PoseLandmark.LEFT_WRIST : \"LEFT_WRIST\",\n",
    "    mp_pose.PoseLandmark.RIGHT_WRIST : \"RIGHT_WRIST\",\n",
    "    mp_pose.PoseLandmark.LEFT_PINKY : \"LEFT_PINKY\",\n",
    "    mp_pose.PoseLandmark.RIGHT_PINKY : \"RIGHT_PINKY\",\n",
    "    mp_pose.PoseLandmark.LEFT_INDEX : \"LEFT_INDEX\",\n",
    "    mp_pose.PoseLandmark.RIGHT_INDEX : \"RIGHT_INDEX\",\n",
    "    mp_pose.PoseLandmark.LEFT_THUMB : \"LEFT_THUMB\",\n",
    "    mp_pose.PoseLandmark.RIGHT_THUMB : \"RIGHT_THUMB\",\n",
    "    mp_pose.PoseLandmark.LEFT_HIP : \"LEFT_HIP\",\n",
    "    mp_pose.PoseLandmark.RIGHT_HIP : \"RIGHT_HIP\",\n",
    "    mp_pose.PoseLandmark.LEFT_KNEE : \"LEFT_KNEE\",\n",
    "    mp_pose.PoseLandmark.RIGHT_KNEE : \"RIGHT_KNEE\",\n",
    "    mp_pose.PoseLandmark.LEFT_ANKLE : \"LEFT_ANKLE\",\n",
    "    mp_pose.PoseLandmark.RIGHT_ANKLE : \"RIGHT_ANKLE\",\n",
    "    mp_pose.PoseLandmark.LEFT_HEEL : \"LEFT_HEEL\",\n",
    "    mp_pose.PoseLandmark.RIGHT_HEEL : \"RIGHT_HEEL\",\n",
    "    mp_pose.PoseLandmark.LEFT_FOOT_INDEX : \"LEFT_FOOT_INDEX\",\n",
    "    mp_pose.PoseLandmark.RIGHT_FOOT_INDEX : \"RIGHT_FOOT_INDEX\"\n",
    "}\n",
    "\n",
    "for lmark in mp_pose.PoseLandmark:\n",
    "    print(landmarkStr[lmark])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NOSE\n",
      "LEFT_EYE_INNER\n",
      "LEFT_EYE\n",
      "LEFT_EYE_OUTER\n",
      "RIGHT_EYE_INNER\n",
      "RIGHT_EYE\n",
      "RIGHT_EYE_OUTER\n",
      "LEFT_EAR\n",
      "RIGHT_EAR\n",
      "MOUTH_LEFT\n",
      "MOUTH_RIGHT\n",
      "LEFT_SHOULDER\n",
      "RIGHT_SHOULDER\n",
      "LEFT_ELBOW\n",
      "RIGHT_ELBOW\n",
      "LEFT_WRIST\n",
      "RIGHT_WRIST\n",
      "LEFT_PINKY\n",
      "RIGHT_PINKY\n",
      "LEFT_INDEX\n",
      "RIGHT_INDEX\n",
      "LEFT_THUMB\n",
      "RIGHT_THUMB\n",
      "LEFT_HIP\n",
      "RIGHT_HIP\n",
      "LEFT_KNEE\n",
      "RIGHT_KNEE\n",
      "LEFT_ANKLE\n",
      "RIGHT_ANKLE\n",
      "LEFT_HEEL\n",
      "RIGHT_HEEL\n",
      "LEFT_FOOT_INDEX\n",
      "RIGHT_FOOT_INDEX\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "x: 0.704304575920105\n",
       "y: 0.8361599445343018\n",
       "z: -0.29689401388168335\n",
       "visibility: 0.9932959079742432"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "straight_arms_keypoints = [mp_pose.PoseLandmark.RIGHT_WRIST, \n",
    "                           mp_pose.PoseLandmark.RIGHT_ELBOW, \n",
    "                           mp_pose.PoseLandmark.RIGHT_SHOULDER, \n",
    "                           mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "                          mp_pose.PoseLandmark.LEFT_ELBOW,\n",
    "                          mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "cords_arms = [(landmarks[kp.value].x, landmarks[kp.value].y, landmarks[kp.value].z) for kp in straight_arms_keypoints]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calc_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)    \n",
    "    c = np.array(c)   \n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 380-angle \n",
    "    \n",
    "    return angle "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "arm_angle = [] \n",
    "for i in range(len(cords_arms)-2):\n",
    "    fst, snd, thrd = cords_arms[i-2], cords_arms[i-1], cords_arms[i]\n",
    "    arm_angle.append(calc_angle(fst, snd, thrd))\n",
    "print(arm_angle)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[73.7300109416294, 94.6913244090384, 163.24095677145465, 105.5763244638607]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "''' Using the media pose model'''\n",
    "import time \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    '''\n",
    "        Capturing webcam footage \n",
    "    '''\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        suc, frame = cap.read() \n",
    "        if not suc:\n",
    "            print(\"Frame empty..\")\n",
    "            continue \n",
    "        \n",
    "        #Recolor image \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detections \n",
    "        results = pose.process(image)\n",
    "        \n",
    "        #Revert image color \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        #Extract Landmarks\n",
    "        try: \n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Find keypoints for straight arm across whole body \n",
    "            straight_arms_keypoints = [mp_pose.PoseLandmark.RIGHT_WRIST, \n",
    "                           mp_pose.PoseLandmark.RIGHT_ELBOW, \n",
    "                           mp_pose.PoseLandmark.RIGHT_SHOULDER, \n",
    "                           mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "                          mp_pose.PoseLandmark.LEFT_ELBOW,\n",
    "                          mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "            \n",
    "            #Get cords for keypoints \n",
    "            cords_arms = [(landmarks[kp.value].x, \n",
    "                           landmarks[kp.value].y) \n",
    "                          for kp in straight_arms_keypoints]\n",
    "            \n",
    "            '''\n",
    "                This doesn't work yet \n",
    "                \n",
    "                Ideally it maps onto each keypoint the angle of that body part - doesn't seem very accurate  \n",
    "                \n",
    "                Moving right arm seems to be affecting left..?\n",
    "            '''\n",
    "            \n",
    "            #Calculate angles \n",
    "            arm_angles = [] \n",
    "            for i in range(1, len(cords_arms)-1):\n",
    "                \n",
    "                fst, snd, thrd = cords_arms[i-1], cords_arms[i], cords_arms[i+1]\n",
    "                \n",
    "                \n",
    "                arm_angles.append(calc_angle(fst, snd, thrd))\n",
    "            \n",
    "            angles_items = ['r_elbow', 'r_shoulder', 'l_shoulder', 'l_elbow']\n",
    "            \n",
    "            #Visualise Right Elbow, Right Shoulder, Left Elbow, Left Shoulder\n",
    "            for i, item in enumerate(angles_items):\n",
    "                # Weird bounding to get the cords of Relbow, Rshoulder, Lelbow, Lshoulder from \n",
    "                # Straight arm keypoints\n",
    "                cv2.putText(image, \n",
    "                            str(round(arm_angles[i], 2)) + item,\n",
    "                            tuple(np.multiply(cords_arms[1:-1][i], [640, 480]).astype(int)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA \n",
    "                            ) \n",
    "                \n",
    "#                 print(str(round(arm_angles[i], 2)) + item)\n",
    "#                 print(cords_arms[1:-1][i])\n",
    "#                 print()\n",
    "#                 time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "         \n",
    "        #Render detections \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(50, 145, 168), circle_radius=2, thickness=2),\n",
    "            mp_drawing.DrawingSpec(color=(209, 192, 42), circle_radius=2, thickness=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Displaying the frame \n",
    "        cv2.imshow(\"Video\", image)\n",
    "        \n",
    "        # Closing the video capture  \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Person():\n",
    "    def __init__(self):\n",
    "        self.landmark_cords = dict()\n",
    "            \n",
    "    def get_left_shoulder(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        \n",
    "    def get_right_shoulder(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        \n",
    "    def get_left_elbow(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        \n",
    "    def get_right_elbow(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "        \n",
    "    def get_left_wrist(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "        \n",
    "    def get_right_wrist(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "        \n",
    "    def get_left_hip(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        \n",
    "    def get_right_hip(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        \n",
    "    def get_left_knee(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "        \n",
    "    def get_right_knee(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        \n",
    "    def get_left_ankle(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "        \n",
    "    def get_right_ankle(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "        \n",
    "    def get_left_heel(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_HEEL]\n",
    "        \n",
    "    def get_right_heel(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_HEEL]\n",
    "        \n",
    "    def get_left_foot_index(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.LEFT_FOOT_INDEX]\n",
    "        \n",
    "    def get_right_foot_index(self):\n",
    "        return self.landmark_cords[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def updatePerson(person, cords):\n",
    "    print(PoseLandmark.LEFT_FOOT_INDEX)\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DRAWING CORDS ON PERSON"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "''' Using the media pose model'''\n",
    "import time \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    '''\n",
    "        Capturing webcam footage \n",
    "    '''\n",
    "    allExceptFaceAndFingers = list(mp_pose.PoseLandmark)[11:17] + list(mp_pose.PoseLandmark)[23:]\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)    \n",
    "    # curPerson = Person()\n",
    "    new_landmark_cords = {landmark : tuple() for landmark in allExceptFaceAndFingers}\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        suc, frame = cap.read() \n",
    "        if not suc:\n",
    "            print(\"Frame empty..\")\n",
    "            continue \n",
    "        \n",
    "        #Recolor image \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detections \n",
    "        results = pose.process(image)\n",
    "        \n",
    "        #Revert image color \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        #Extract Landmarks\n",
    "        try: \n",
    "            cur_landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            #Normalised X, Y Cord for current landmark \n",
    "            cur_landmark_cords = [(cur_landmarks[kp.value].x, cur_landmarks[kp.value].y) for kp in allExceptFaceAndFingers]\n",
    "            \n",
    "            for idx, landmark_cord in enumerate(cur_landmark_cords):\n",
    "                curLandmarkStr = landmarkStr[allExceptFaceAndFingers[idx]]\n",
    "                real_cords = tuple(np.multiply(landmark_cord, [640, 480]).astype(int))\n",
    "                \n",
    "                cv2.putText(image, f'{curLandmarkStr} Pos: {real_cords}', real_cords, \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA) \n",
    "                \n",
    "                # Normalized Cord for the feature \n",
    "                new_landmark_cords[allExceptFaceAndFingers[idx]] = landmark_cord\n",
    "            \n",
    "            #Update cords of person object\n",
    "            # updatePerson(curPerson, new_landmark_cords)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "         \n",
    "        #Render detections \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(50, 145, 168), circle_radius=2, thickness=2),\n",
    "            mp_drawing.DrawingSpec(color=(209, 192, 42), circle_radius=2, thickness=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Displaying the frame \n",
    "        cv2.imshow(\"Video\", image)\n",
    "        \n",
    "        # Closing the video capture  \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n",
      "name 'landmarkStr' is not defined\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating person object "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "RelevantLandmarks = list(mp_pose.PoseLandmark)[11:17] + list(mp_pose.PoseLandmark)[23:]\n",
    "RelevantLandmarks"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<PoseLandmark.LEFT_SHOULDER: 11>,\n",
       " <PoseLandmark.RIGHT_SHOULDER: 12>,\n",
       " <PoseLandmark.LEFT_ELBOW: 13>,\n",
       " <PoseLandmark.RIGHT_ELBOW: 14>,\n",
       " <PoseLandmark.LEFT_WRIST: 15>,\n",
       " <PoseLandmark.RIGHT_WRIST: 16>,\n",
       " <PoseLandmark.LEFT_HIP: 23>,\n",
       " <PoseLandmark.RIGHT_HIP: 24>,\n",
       " <PoseLandmark.LEFT_KNEE: 25>,\n",
       " <PoseLandmark.RIGHT_KNEE: 26>,\n",
       " <PoseLandmark.LEFT_ANKLE: 27>,\n",
       " <PoseLandmark.RIGHT_ANKLE: 28>,\n",
       " <PoseLandmark.LEFT_HEEL: 29>,\n",
       " <PoseLandmark.RIGHT_HEEL: 30>,\n",
       " <PoseLandmark.LEFT_FOOT_INDEX: 31>,\n",
       " <PoseLandmark.RIGHT_FOOT_INDEX: 32>]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "''' Using the media pose model'''\n",
    "import time \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    '''\n",
    "        Capturing webcam footage \n",
    "    '''\n",
    "    allExceptFaceAndFingers = list(mp_pose.PoseLandmark)[11:17] + list(mp_pose.PoseLandmark)[23:]\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)    \n",
    "    curPerson = Person()\n",
    "    new_landmark_cords = {landmark : tuple() for landmark in allExceptFaceAndFingers}\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        suc, frame = cap.read() \n",
    "        if not suc:\n",
    "            print(\"Frame empty..\")\n",
    "            continue \n",
    "        \n",
    "        #Recolor image \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detections \n",
    "        results = pose.process(image)\n",
    "        \n",
    "        #Revert image color \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        #Extract Landmarks\n",
    "        try: \n",
    "            cur_landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            #Normalised X, Y Cord for current landmark \n",
    "            cur_landmark_cords = [(cur_landmarks[kp.value].x, ur_landmarks[kp.value].y) for kp in allExceptFaceAndFingers]\n",
    "            \n",
    "            for idx, landmark_cord in enumerate(cur_landmark_cords):\n",
    "                curLandmarkStr = landmarkStr[allExceptFaceAndFingers[idx]]\n",
    "                real_cords = tuple(np.multiply(landmark_cord, [640, 480]).astype(int))\n",
    "                \n",
    "                cv2.putText(image, f'{curLandmarkStr} Pos: {real_cords}', real_cords, \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA) \n",
    "                \n",
    "                # Normalized Cord for the feature \n",
    "                new_landmark_cords[allExceptFaceAndFingers[idx]] = landmark_cord\n",
    "            \n",
    "            #Update cords of person object\n",
    "            updatePerson(curPerson, new_landmark_cords)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "         \n",
    "        #Render detections \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(50, 145, 168), circle_radius=2, thickness=2),\n",
    "            mp_drawing.DrawingSpec(color=(209, 192, 42), circle_radius=2, thickness=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Displaying the frame \n",
    "        cv2.imshow(\"Video\", image)\n",
    "        \n",
    "        # Closing the video capture  \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Person' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m allExceptFaceAndFingers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(mp_pose\u001b[39m.\u001b[39mPoseLandmark)[\u001b[39m11\u001b[39m:\u001b[39m17\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(mp_pose\u001b[39m.\u001b[39mPoseLandmark)[\u001b[39m23\u001b[39m:]\n\u001b[0;32m      9\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)    \n\u001b[1;32m---> 10\u001b[0m curPerson \u001b[39m=\u001b[39m Person()\n\u001b[0;32m     11\u001b[0m new_landmark_cords \u001b[39m=\u001b[39m {landmark : \u001b[39mtuple\u001b[39m() \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m allExceptFaceAndFingers}\n\u001b[0;32m     13\u001b[0m \u001b[39mwhile\u001b[39;00m cap\u001b[39m.\u001b[39misOpened():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Person' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.8 64-bit ('3yp_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c03d7ca79527c43668de5fef191e6a32e993596d47f3147d82e5050d734e1650"
   }
  },
  "interpreter": {
   "hash": "51db6f61e71e45afcb9d39d094a10a56b906d450dc2fdab44f391ee8a2ffeda1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}